{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KUf1cFDsDToX",
    "outputId": "4cbbb739-9783-449d-8c28-e79ec3e27b4d",
    "ExecuteTime": {
     "end_time": "2025-07-07T22:50:43.235629Z",
     "start_time": "2025-07-07T22:50:43.223581Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pygad\n",
    "from scipy.optimize import differential_evolution\n",
    "import matplotlib.pyplot as plt\n"
   ],
   "outputs": [],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "source": [
    "# Load CCPP dataset (make sure you have 'Folds5x2_pp.csv' locally or download from UCI)\n",
    "data = pd.read_csv(\"Folds5x2_pp.csv\")\n",
    "X = data.drop(columns=\"PE\").values\n",
    "y = data[\"PE\"].values\n",
    "\n",
    "# Split into train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ],
   "metadata": {
    "id": "EodFacvNDb3a",
    "ExecuteTime": {
     "end_time": "2025-07-07T22:50:43.333269Z",
     "start_time": "2025-07-07T22:50:43.282342Z"
    }
   },
   "outputs": [],
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, n_hidden, neuron_list):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        in_features = input_dim\n",
    "        for i in range(n_hidden):\n",
    "            out_features = neuron_list[i]\n",
    "            layers.append(nn.Linear(in_features, out_features))\n",
    "            layers.append(nn.ReLU())\n",
    "            in_features = out_features\n",
    "        layers.append(nn.Linear(in_features, 1))  # Output layer\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "4Gw8mVhTRRfj",
    "ExecuteTime": {
     "end_time": "2025-07-07T22:50:43.374794Z",
     "start_time": "2025-07-07T22:50:43.361136Z"
    }
   },
   "outputs": [],
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "source": [
    "def fitness_func(ga_instance, solution, solution_idx):\n",
    "    import torch\n",
    "    from torch import nn\n",
    "    from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    print(device)\n",
    "    n_hidden = int(round(solution[0]))\n",
    "    neurons = [int(round(n)) for n in solution[1:6]]\n",
    "    log_lr = solution[6]\n",
    "    learning_rate = 10 ** log_lr\n",
    "\n",
    "    try:\n",
    "        model = MLP(X_train.shape[1], n_hidden, neurons).to(device)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Dataset\n",
    "        X_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "        y_tensor = torch.tensor(y_train.reshape(-1, 1), dtype=torch.float32)\n",
    "        dataset = TensorDataset(X_tensor, y_tensor)\n",
    "\n",
    "        # 80/20 split\n",
    "        train_size = int(0.8 * len(dataset))\n",
    "        val_size = len(dataset) - train_size\n",
    "        train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "        best_val_loss = float(\"inf\")\n",
    "\n",
    "        for epoch in range(20):  # match TensorFlow: epochs=20\n",
    "            model.train()\n",
    "            for xb, yb in train_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                loss = criterion(model(xb), yb)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Validation\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for xb, yb in val_loader:\n",
    "                    xb, yb = xb.to(device), yb.to(device)\n",
    "                    val_loss += criterion(model(xb), yb).item()\n",
    "            val_loss /= len(val_loader)\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "\n",
    "        print(f\"[{solution_idx}] layers={n_hidden}, neurons={neurons[:n_hidden]}, lr={learning_rate:.1e}, val_loss={best_val_loss:.4f}\")\n",
    "        return -best_val_loss\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[{solution_idx}] FAILED: {e}\")\n",
    "        return -1e6\n"
   ],
   "metadata": {
    "id": "Nve5KKSxdCm-",
    "ExecuteTime": {
     "end_time": "2025-07-07T22:50:43.433528Z",
     "start_time": "2025-07-07T22:50:43.406026Z"
    }
   },
   "outputs": [],
   "execution_count": 38
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "eUFSwaCJdFAY"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "fitness_per_gen = []\n",
    "def on_generation(ga_instance):\n",
    "    fitness_per_gen.append(np.max(ga_instance.last_generation_fitness))\n",
    "ga_instance = pygad.GA(\n",
    "    num_generations=10,\n",
    "    num_parents_mating=5,\n",
    "    fitness_func=fitness_func,\n",
    "    sol_per_pop=10,\n",
    "    num_genes=7,\n",
    "    on_generation=on_generation,\n",
    "    gene_space=[\n",
    "        {'low': 1, 'high': 5},        # n_hidden\n",
    "        {'low': 10, 'high': 200},     # n1\n",
    "        {'low': 10, 'high': 200},     # n2\n",
    "        {'low': 10, 'high': 200},     # n3\n",
    "        {'low': 10, 'high': 200},     # n4\n",
    "        {'low': 10, 'high': 200},     # n5\n",
    "        {'low': -4, 'high': -2}       # log10(lr)\n",
    "    ],\n",
    "    mutation_percent_genes=20,\n",
    "    random_seed=42\n",
    ")\n",
    "\n",
    "ga_instance.run()\n",
    "best_solution, _, _ = ga_instance.best_solution()\n",
    "print(\"GA best solution:\", best_solution)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_YGA3h0WdFcW",
    "outputId": "d246ef1b-5054-4de5-ecc5-52a58e7352a0",
    "ExecuteTime": {
     "end_time": "2025-07-07T23:02:07.794804Z",
     "start_time": "2025-07-07T22:50:43.462748Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n",
      "[0] layers=2, neurons=[191, 149], lr=1.3e-04, val_loss=7951.4125\n",
      "mps\n",
      "[1] layers=4, neurons=[124, 145, 14, 194], lr=2.7e-04, val_loss=68.2979\n",
      "mps\n",
      "[2] layers=2, neurons=[45, 68], lr=1.7e-03, val_loss=54.3656\n",
      "mps\n",
      "[3] layers=2, neurons=[66, 80], lr=1.1e-03, val_loss=153.8862\n",
      "mps\n",
      "[4] layers=3, neurons=[19, 125, 42], lr=8.5e-03, val_loss=18.9664\n",
      "mps\n",
      "[5] layers=4, neurons=[68, 29, 140, 94], lr=9.8e-04, val_loss=36.0448\n",
      "mps\n",
      "[6] layers=1, neurons=[183], lr=1.2e-03, val_loss=1431.0320\n",
      "mps\n",
      "[7] layers=2, neurons=[194, 157], lr=7.0e-03, val_loss=17.4525\n",
      "mps\n",
      "[8] layers=1, neurons=[47], lr=4.5e-03, val_loss=80.0933\n",
      "mps\n",
      "[9] layers=2, neurons=[63, 113], lr=9.4e-03, val_loss=21.3881\n",
      "mps\n",
      "[1] layers=3, neurons=[19, 125, 42], lr=8.5e-03, val_loss=20.4591\n",
      "mps\n",
      "[2] layers=4, neurons=[19, 125, 37, 162], lr=9.4e-03, val_loss=18.8059\n",
      "mps\n",
      "[3] layers=3, neurons=[63, 113, 37], lr=9.8e-04, val_loss=34.8907\n",
      "mps\n",
      "[4] layers=4, neurons=[68, 68, 110, 92], lr=1.7e-03, val_loss=23.8914\n",
      "mps\n",
      "[5] layers=2, neurons=[45, 158], lr=7.0e-03, val_loss=20.5020\n",
      "mps\n",
      "[6] layers=3, neurons=[19, 125, 42], lr=8.5e-03, val_loss=19.8976\n",
      "mps\n",
      "[7] layers=3, neurons=[18, 113, 37], lr=9.4e-03, val_loss=20.6465\n",
      "mps\n",
      "[8] layers=2, neurons=[63, 29], lr=9.8e-04, val_loss=521.1635\n",
      "mps\n",
      "[9] layers=2, neurons=[45, 68], lr=1.7e-03, val_loss=90.6797\n",
      "mps\n",
      "[1] layers=3, neurons=[194, 157, 189], lr=9.4e-03, val_loss=17.4958\n",
      "mps\n",
      "[2] layers=4, neurons=[19, 125, 42, 22], lr=8.5e-03, val_loss=16.9933\n",
      "mps\n",
      "[3] layers=3, neurons=[19, 125, 42], lr=8.5e-03, val_loss=19.0779\n",
      "mps\n",
      "[4] layers=3, neurons=[19, 125, 42], lr=7.0e-03, val_loss=18.9526\n",
      "mps\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[39]\u001B[39m\u001B[32m, line 24\u001B[39m\n\u001B[32m      3\u001B[39m     fitness_per_gen.append(np.max(ga_instance.last_generation_fitness))\n\u001B[32m      4\u001B[39m ga_instance = pygad.GA(\n\u001B[32m      5\u001B[39m     num_generations=\u001B[32m10\u001B[39m,\n\u001B[32m      6\u001B[39m     num_parents_mating=\u001B[32m5\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m     21\u001B[39m     random_seed=\u001B[32m42\u001B[39m\n\u001B[32m     22\u001B[39m )\n\u001B[32m---> \u001B[39m\u001B[32m24\u001B[39m \u001B[43mga_instance\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     25\u001B[39m best_solution, _, _ = ga_instance.best_solution()\n\u001B[32m     26\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mGA best solution:\u001B[39m\u001B[33m\"\u001B[39m, best_solution)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/IdeaProjects/robotika/.venv/lib/python3.12/site-packages/pygad/pygad.py:1952\u001B[39m, in \u001B[36mGA.run\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1950\u001B[39m \u001B[38;5;28mself\u001B[39m.previous_generation_fitness = \u001B[38;5;28mself\u001B[39m.last_generation_fitness.copy()\n\u001B[32m   1951\u001B[39m \u001B[38;5;66;03m# Measuring the fitness of each chromosome in the population. Save the fitness in the last_generation_fitness attribute.\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1952\u001B[39m \u001B[38;5;28mself\u001B[39m.last_generation_fitness = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mcal_pop_fitness\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1954\u001B[39m best_solution, best_solution_fitness, best_match_idx = \u001B[38;5;28mself\u001B[39m.best_solution(\n\u001B[32m   1955\u001B[39m     pop_fitness=\u001B[38;5;28mself\u001B[39m.last_generation_fitness)\n\u001B[32m   1957\u001B[39m \u001B[38;5;66;03m# Appending the best solution in the current generation to the best_solutions list.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/IdeaProjects/robotika/.venv/lib/python3.12/site-packages/pygad/pygad.py:1697\u001B[39m, in \u001B[36mGA.cal_pop_fitness\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1694\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1695\u001B[39m     \u001B[38;5;66;03m# Check if batch processing is used. If not, then calculate this missing fitness value.\u001B[39;00m\n\u001B[32m   1696\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.fitness_batch_size \u001B[38;5;129;01min\u001B[39;00m [\u001B[32m1\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m]:\n\u001B[32m-> \u001B[39m\u001B[32m1697\u001B[39m         fitness = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfitness_func\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msol\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msol_idx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1698\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(fitness) \u001B[38;5;129;01min\u001B[39;00m GA.supported_int_float_types:\n\u001B[32m   1699\u001B[39m             \u001B[38;5;66;03m# The fitness function returns a single numeric value.\u001B[39;00m\n\u001B[32m   1700\u001B[39m             \u001B[38;5;66;03m# This is a single-objective optimization problem.\u001B[39;00m\n\u001B[32m   1701\u001B[39m             \u001B[38;5;28;01mpass\u001B[39;00m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[38]\u001B[39m\u001B[32m, line 40\u001B[39m, in \u001B[36mfitness_func\u001B[39m\u001B[34m(ga_instance, solution, solution_idx)\u001B[39m\n\u001B[32m     38\u001B[39m     loss = criterion(model(xb), yb)\n\u001B[32m     39\u001B[39m     loss.backward()\n\u001B[32m---> \u001B[39m\u001B[32m40\u001B[39m     \u001B[43moptimizer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     42\u001B[39m \u001B[38;5;66;03m# Validation\u001B[39;00m\n\u001B[32m     43\u001B[39m model.eval()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/IdeaProjects/robotika/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py:385\u001B[39m, in \u001B[36mOptimizer.profile_hook_step.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    380\u001B[39m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    381\u001B[39m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[32m    382\u001B[39m                 \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m must return None or a tuple of (new_args, new_kwargs), but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresult\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    383\u001B[39m             )\n\u001B[32m--> \u001B[39m\u001B[32m385\u001B[39m out = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    386\u001B[39m \u001B[38;5;28mself\u001B[39m._optimizer_step_code()\n\u001B[32m    388\u001B[39m \u001B[38;5;66;03m# call optimizer step post hooks\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/IdeaProjects/robotika/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py:76\u001B[39m, in \u001B[36m_use_grad_for_differentiable.<locals>._use_grad\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m     74\u001B[39m     torch.set_grad_enabled(\u001B[38;5;28mself\u001B[39m.defaults[\u001B[33m'\u001B[39m\u001B[33mdifferentiable\u001B[39m\u001B[33m'\u001B[39m])\n\u001B[32m     75\u001B[39m     torch._dynamo.graph_break()\n\u001B[32m---> \u001B[39m\u001B[32m76\u001B[39m     ret = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     77\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m     78\u001B[39m     torch._dynamo.graph_break()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/IdeaProjects/robotika/.venv/lib/python3.12/site-packages/torch/optim/adam.py:166\u001B[39m, in \u001B[36mAdam.step\u001B[39m\u001B[34m(self, closure)\u001B[39m\n\u001B[32m    155\u001B[39m     beta1, beta2 = group[\u001B[33m'\u001B[39m\u001B[33mbetas\u001B[39m\u001B[33m'\u001B[39m]\n\u001B[32m    157\u001B[39m     has_complex = \u001B[38;5;28mself\u001B[39m._init_group(\n\u001B[32m    158\u001B[39m         group,\n\u001B[32m    159\u001B[39m         params_with_grad,\n\u001B[32m   (...)\u001B[39m\u001B[32m    163\u001B[39m         max_exp_avg_sqs,\n\u001B[32m    164\u001B[39m         state_steps)\n\u001B[32m--> \u001B[39m\u001B[32m166\u001B[39m     \u001B[43madam\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    167\u001B[39m \u001B[43m        \u001B[49m\u001B[43mparams_with_grad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    168\u001B[39m \u001B[43m        \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    169\u001B[39m \u001B[43m        \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    170\u001B[39m \u001B[43m        \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    171\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    172\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    173\u001B[39m \u001B[43m        \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mamsgrad\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    174\u001B[39m \u001B[43m        \u001B[49m\u001B[43mhas_complex\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhas_complex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    175\u001B[39m \u001B[43m        \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    176\u001B[39m \u001B[43m        \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    177\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlr\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mlr\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    178\u001B[39m \u001B[43m        \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mweight_decay\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    179\u001B[39m \u001B[43m        \u001B[49m\u001B[43meps\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43meps\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    180\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mmaximize\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    181\u001B[39m \u001B[43m        \u001B[49m\u001B[43mforeach\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mforeach\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    182\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mcapturable\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    183\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mdifferentiable\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    184\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfused\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mfused\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    185\u001B[39m \u001B[43m        \u001B[49m\u001B[43mgrad_scale\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mgrad_scale\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    186\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mfound_inf\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    187\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    189\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m loss\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/IdeaProjects/robotika/.venv/lib/python3.12/site-packages/torch/optim/adam.py:316\u001B[39m, in \u001B[36madam\u001B[39m\u001B[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001B[39m\n\u001B[32m    313\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    314\u001B[39m     func = _single_tensor_adam\n\u001B[32m--> \u001B[39m\u001B[32m316\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    317\u001B[39m \u001B[43m     \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    318\u001B[39m \u001B[43m     \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    319\u001B[39m \u001B[43m     \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    320\u001B[39m \u001B[43m     \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    321\u001B[39m \u001B[43m     \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    322\u001B[39m \u001B[43m     \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[43m=\u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    323\u001B[39m \u001B[43m     \u001B[49m\u001B[43mhas_complex\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhas_complex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    324\u001B[39m \u001B[43m     \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    325\u001B[39m \u001B[43m     \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    326\u001B[39m \u001B[43m     \u001B[49m\u001B[43mlr\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    327\u001B[39m \u001B[43m     \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[43m=\u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    328\u001B[39m \u001B[43m     \u001B[49m\u001B[43meps\u001B[49m\u001B[43m=\u001B[49m\u001B[43meps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    329\u001B[39m \u001B[43m     \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmaximize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    330\u001B[39m \u001B[43m     \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcapturable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    331\u001B[39m \u001B[43m     \u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    332\u001B[39m \u001B[43m     \u001B[49m\u001B[43mgrad_scale\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgrad_scale\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    333\u001B[39m \u001B[43m     \u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/IdeaProjects/robotika/.venv/lib/python3.12/site-packages/torch/optim/adam.py:441\u001B[39m, in \u001B[36m_single_tensor_adam\u001B[39m\u001B[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001B[39m\n\u001B[32m    438\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    439\u001B[39m         denom = (exp_avg_sq.sqrt() / bias_correction2_sqrt).add_(eps)\n\u001B[32m--> \u001B[39m\u001B[32m441\u001B[39m     \u001B[43mparam\u001B[49m\u001B[43m.\u001B[49m\u001B[43maddcdiv_\u001B[49m\u001B[43m(\u001B[49m\u001B[43mexp_avg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdenom\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m=\u001B[49m\u001B[43m-\u001B[49m\u001B[43mstep_size\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    443\u001B[39m \u001B[38;5;66;03m# Lastly, switch back to complex view\u001B[39;00m\n\u001B[32m    444\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m amsgrad \u001B[38;5;129;01mand\u001B[39;00m torch.is_complex(params[i]):\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(fitness_per_gen, marker='s', linestyle='--', color='darkorange', label='Best Per Generation')\n",
    "plt.title(\"Best Validation Loss Per Generation\")\n",
    "plt.xlabel(\"Generation\")\n",
    "plt.ylabel(\"Best Validation Loss (MSE)\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.show()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "id": "yOv_xchjyxkA",
    "outputId": "9883eb52-7c71-47ab-85e7-da58b2da27ab"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "fitness_over_time = []\n",
    "def objective(params):\n",
    "    try:\n",
    "        n_hidden = int(round(params[0]))\n",
    "        neurons = [int(round(p)) for p in params[1:6]]\n",
    "        log_lr = params[6]\n",
    "        learning_rate = 10 ** log_lr\n",
    "\n",
    "        model = MLP(input_dim=X_train.shape[1], hidden_sizes=neurons[:n_hidden])\n",
    "        model.to(device)\n",
    "\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "        X_torch = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "        y_torch = torch.tensor(y_train, dtype=torch.float32).view(-1, 1).to(device)\n",
    "\n",
    "        val_split = 0.2\n",
    "        val_size = int(len(X_torch) * val_split)\n",
    "        train_X, val_X = X_torch[:-val_size], X_torch[-val_size:]\n",
    "        train_y, val_y = y_torch[:-val_size], y_torch[-val_size:]\n",
    "\n",
    "        batch_size = 64\n",
    "        train_dataset = torch.utils.data.TensorDataset(train_X, train_y)\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        best_val_loss = float('inf')\n",
    "        epochs_no_improve = 0\n",
    "        patience = 5\n",
    "\n",
    "        for epoch in range(10):  # match TF version\n",
    "            model.train()\n",
    "            for xb, yb in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                preds = model(xb)\n",
    "                loss = criterion(preds, yb)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_preds = model(val_X)\n",
    "                val_loss = criterion(val_preds, val_y).item()\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                epochs_no_improve = 0\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "                if epochs_no_improve >= patience:\n",
    "                    break\n",
    "\n",
    "        print(f\"DE: layers={n_hidden}, neurons={neurons[:n_hidden]}, lr={learning_rate:.1e}, val_loss={best_val_loss:.4f}\")\n",
    "        return best_val_loss\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"DE FAILED: {e}\")\n",
    "        return 1e6\n",
    "\n"
   ],
   "metadata": {
    "id": "9yefObIZRS70"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "bounds = [\n",
    "    (1, 5),          # n_hidden\n",
    "    (10, 200),       # n1\n",
    "    (10, 200),       # n2\n",
    "    (10, 200),       # n3\n",
    "    (10, 200),       # n4\n",
    "    (10, 200),       # n5\n",
    "    (-4, -2)         # log10(lr)\n",
    "]\n",
    "\n",
    "# Differential Evolution with GA seed\n",
    "result = differential_evolution(\n",
    "    objective,\n",
    "    bounds,\n",
    "    strategy=\"best1bin\",\n",
    "    maxiter=2,\n",
    "    popsize=5,\n",
    "    seed=42,\n",
    "    x0=best_solution,\n",
    "    disp=True,\n",
    "    polish=False\n",
    ")\n",
    "\n",
    "best_params = result.x\n",
    "print(\"Refined DE solution:\", best_params)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZXHlFpscRVeF",
    "outputId": "5201d9d5-a4c3-4cd1-b5b0-c7b6cb2727ad"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(fitness_over_time, marker='o', linestyle='-', label='Validation Loss')\n",
    "plt.title(\"Validation Loss Over DE Evaluations\")\n",
    "plt.xlabel(\"Evaluation Step\")\n",
    "plt.ylabel(\"Validation Loss (MSE)\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.show()\n"
   ],
   "metadata": {
    "id": "aDgF54cqyh8e"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Unpack final parameters\n",
    "n_hidden_best = int(round(best_params[0]))\n",
    "neurons_best = [int(round(p)) for p in best_params[1:6]]\n",
    "log_lr_best = best_params[6]\n",
    "learning_rate_best = 10 ** log_lr_best\n",
    "\n",
    "print(f\"Final architecture: layers={n_hidden_best}, neurons={neurons_best[:n_hidden_best]}, lr={learning_rate_best:.1e}\")\n",
    "\n",
    "# Final model training\n",
    "final_model = create_mlp(n_hidden_best, neurons_best, learning_rate_best)\n",
    "final_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    verbose=1,\n",
    "    callbacks=[keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)]\n",
    ")\n",
    "\n",
    "# Evaluate on test set\n",
    "mse_test = final_model.evaluate(X_test, y_test, verbose=0)\n",
    "rmse_test = np.sqrt(mse_test)\n",
    "print(f\"Final test MSE: {mse_test:.4f}, RMSE: {rmse_test:.2f} MW\")\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "y_pred = final_model.predict(X_test).flatten()\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"R² score: {r2:.4f}\")\n",
    "\n"
   ],
   "metadata": {
    "id": "njO2yA4FRW-G"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "final_model.save(\"ccpp_best_model.h5\")\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Predictions and R²\n",
    "y_pred = final_model.predict(X_test).flatten()\n",
    "rmse = np.sqrt(np.mean((y_test - y_pred)**2))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5, s=20, c=\"dodgerblue\", edgecolors=\"k\", linewidths=0.3)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', linewidth=1.5)\n",
    "\n",
    "# Labels and title\n",
    "plt.title(f\"CCPP: Actual vs Predicted Power Output\\nRMSE = {rmse:.2f} MW | R² = {r2:.4f}\", fontsize=13)\n",
    "plt.xlabel(\"Actual PE (MW)\", fontsize=12)\n",
    "plt.ylabel(\"Predicted PE (MW)\", fontsize=12)\n",
    "\n",
    "# Ticks and limits\n",
    "plt.xlim(425, 500)\n",
    "plt.ylim(425, 500)\n",
    "plt.xticks(np.arange(430, 501, 10))\n",
    "plt.yticks(np.arange(430, 501, 10))\n",
    "\n",
    "# Grid and layout\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ],
   "metadata": {
    "id": "Zl-uA6l8l6T0"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "7k56eUZcl72K"
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
